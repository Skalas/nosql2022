
* Ejercicio de carga PostgreSQL VS MonetDB
** Postgres Run

#+begin_src shell
  docker exec -it postgres psql -d postgres -Upostgres
  docker exec -it postgres /bin/bash

#+end_src

Vamos a poner a MonetDB a jugar carreras VS PostgreSQL para inserci√≥n de datos.

Tengo una BD de viajes de Ecobici de 2010 a 2017. *‚ö†Ô∏è Son m√°s de 45 millones de registros, osea alrededor de 18GB ‚ö†Ô∏è*, entonces para efectos did√°cticos no conviene que manejen ni intenten cargar esta base completa, por lo que les voy a compartir [[https://drive.google.com/file/d/1FQndpift2iQUc5JHoe1TMZwI0k-wQ4RY/view?usp=sharing][un subset de 10M de registros]].

Para ambos vamos a usar una utiler√≠a de carga masiva de DBeaver, mientras que para MonetDB usaremos el comando COPY, que tambi√©n sirve para carga masiva.

Primero debemos crear tanto en MonetDB como en PostgreSQL la siguiente tabla:

Primero en PostgreSQL:
#+begin_src sql
  create schema ecobici;

  set search_path to ecobici;

  create table ecobici_historico (
    genero_usuario  VARCHAR(80),
    edad_usuario  VARCHAR(80),
    bici  VARCHAR(80),
    fecha_retiro  VARCHAR(80),
    hora_retiro_copy  VARCHAR(80),
    fecha_retiro_completa  VARCHAR(80),
    anio_retiro  VARCHAR(80),
    mes_retiro  VARCHAR(80),
    dia_semana_retiro  VARCHAR(80),
    hora_retiro  VARCHAR(80),
    minuto_retiro  VARCHAR(80),
    segundo_retiro  VARCHAR(80),
    ciclo_estacion_retiro  VARCHAR(80),
    nombre_estacion_retiro  VARCHAR(80),
    direccion_estacion_retiro  VARCHAR(80),
    cp_retiro  VARCHAR(80),
    colonia_retiro  VARCHAR(80),
    codigo_colonia_retiro  VARCHAR(80),
    delegacion_retiro  VARCHAR(80),
    delegacion_retiro_num  VARCHAR(80),
    fecha_arribo  VARCHAR(80),
    hora_arribo_copy  VARCHAR(80),
    fecha_arribo_completa  VARCHAR(80),
    anio_arribo  VARCHAR(80),
    mes_arribo  VARCHAR(80),
    dia_semana_arribo  VARCHAR(80),
    hora_arribo  VARCHAR(80),
    minuto_arribo  VARCHAR(80),
    segundo_arribo  VARCHAR(80),
    ciclo_estacion_arribo  VARCHAR(80),
    nombre_estacion_arribo  VARCHAR(80),
    direccion_estacion_arribo  VARCHAR(80),
    cp_arribo  VARCHAR(80),
    colonia_arribo  VARCHAR(80),
    codigo_colonia_arribo  VARCHAR(80),
    delegacion_arribo  VARCHAR(80),
    delegacion_arribo_num  VARCHAR(80),
    duracion_viaje  VARCHAR(80),
    duracion_viaje_horas  VARCHAR(80),
    duracion_viaje_minutos  VARCHAR(80)
  );
#+end_src

Luego en MonetDB:

En la consola:
#+begin_src sh
docker exec monetdb monetdb create -p monetdb ecobici
docker exec -it monetdb  mclient -u monetdb -d ecobici
#+end_src

Luego dentro del cliente de MonetDB ~mclient~ creamos el usuario y el esquema ~ecobici~:

#+begin_src sql
  CREATE USER "ecobici" WITH PASSWORD 'ecobici' NAME 'EcoBici Explorer' SCHEMA "sys";
  CREATE SCHEMA "ecobici" AUTHORIZATION "ecobici";
  ALTER USER "ecobici" SET SCHEMA "ecobici";
  \q
#+end_src

Y finalmente entramos con el usuario ~ecobici~ que acabamos de crear y creamos la tabla ~ecobici_historico~:

#+begin_src sh
docker exec -it monetdb  mclient -u ecobici -d ecobici
  password: "ecobici"
#+end_src

Ya dentro de ~mclient~, ejecutamos:

#+begin_src sql
  create table ecobici_historico (
    genero_usuario  VARCHAR(80),
    edad_usuario  VARCHAR(80),
    bici  VARCHAR(80),
    fecha_retiro  VARCHAR(80),
    hora_retiro_copy  VARCHAR(80),
    fecha_retiro_completa  VARCHAR(80),
    anio_retiro  VARCHAR(80),
    mes_retiro  VARCHAR(80),
    dia_semana_retiro  VARCHAR(80),
    hora_retiro  VARCHAR(80),
    minuto_retiro  VARCHAR(80),
    segundo_retiro  VARCHAR(80),
    ciclo_estacion_retiro  VARCHAR(80),
    nombre_estacion_retiro  VARCHAR(80),
    direccion_estacion_retiro  VARCHAR(80),
    cp_retiro  VARCHAR(80),
    colonia_retiro  VARCHAR(80),
    codigo_colonia_retiro  VARCHAR(80),
    delegacion_retiro  VARCHAR(80),
    delegacion_retiro_num  VARCHAR(80),
    fecha_arribo  VARCHAR(80),
    hora_arribo_copy  VARCHAR(80),
    fecha_arribo_completa  VARCHAR(80),
    anio_arribo  VARCHAR(80),
    mes_arribo  VARCHAR(80),
    dia_semana_arribo  VARCHAR(80),
    hora_arribo  VARCHAR(80),
    minuto_arribo  VARCHAR(80),
    segundo_arribo  VARCHAR(80),
    ciclo_estacion_arribo  VARCHAR(80),
    nombre_estacion_arribo  VARCHAR(80),
    direccion_estacion_arribo  VARCHAR(80),
    cp_arribo  VARCHAR(80),
    colonia_arribo  VARCHAR(80),
    codigo_colonia_arribo  VARCHAR(80),
    delegacion_arribo  VARCHAR(80),
    delegacion_arribo_num  VARCHAR(80),
    duracion_viaje  VARCHAR(80),
    duracion_viaje_horas  VARCHAR(80),
    duracion_viaje_minutos  VARCHAR(80)
  );
#+end_src

#+begin_src sh
  docker exec -ti monetdb mclient -u ecobici -d ecobici
#+end_src

Podemos cargar los csv's desde DBeaver... sin embargo:
**‚ö†Ô∏è ESTA NO ES LA SOLUCI√ìN M√ÅS √ìPTIMA!**

La herramienta que nos da DBeaver *NO ES* la forma m√°s √≥ptima de hacer cargas masivas a BDs, ni columnares ni relacionales.

La forma m√°s √≥ptima es el comando ~COPY~.

El ~COPY~ realiza bastantes optimizaciones tanto del lado de la BD como del sistema operativo para poder realizar estas cargas.

Lamentablemente, son herramientas S√öPER PICKY!

** Postgres

1. El ~COPY~ puede ser invocado desde una herramienta SQL como una ventana de DBeaver, o desde la l√≠nea de comandos.
2. Cuando se invoca desde la l√≠nea de comandos, se hace en conjunto con el comando ~psql~, que es el command-line de PostgreSQL, y entonces el ~copy~ se vuelve el _metacomando_ ~\copy~.
3. La sintaxis general para este caso de ecobici es ~copy ecobici_historico from '/ruta/al/archivo/ecobici_2010_2017-final.csv' with csv header~. Si esto lo corremos desde ~psql~, entonces debemos de anteponer el ~\~ al ~copy~
   - la parte de ~with csv header~ le dice al copy que la entrada es un archivo CSV y que adem√°s la 1a l√≠nea tiene los nombres de las columnas.
4. Lo invoquemos por donde lo invoquemos, el PostgreSQL hace uso de una funci√≥n *VIEJ√çSIMA* del sistema operativo llamada ~fstat()~ que sirve para saber si un argumento es archivo o es directorio.
   - Esta funci√≥n existe desde los sistemas operativos antecesores del Windows y nunca se ha actualizado porque ya todos los lenguajes de programaci√≥n tienen sus propias funciones para obtener esta respuesta.
5. Esta funci√≥n, vieja como es, no admite como argumento archvivos gigantes de m√°s de 4GB.


#+begin_src shell
     cat ecobici_2010_2017-mitad.csv |\
         docker exec -i \
                postgres psql -U postgres\
                -c "copy ecobici.ecobici_historico from stdin (delimiter ',') "
#+end_src


** Monetdb
1. La sintaxis del ~copy~ en MonetDB es similar. En general es ~copy offset 2 into ecobici_historico from '/ruta/al/archivo/ecobici_2010_2017-final.csv' on client using delimiters ',',E'\r' null as ' ';~

   #+begin_src shell
     copy offset 2 into ecobici_historico from '/tmp/new/ecobici_2010_2017-mitad.csv' on client using delimiters ',',E'\n',E'\"' null as ' ';
   #+end_src
   - el ~offset 2~ es para indicar que el 1er rengl√≥n no lo debemos procesar porque son los encabezados de las columnas.
   - el ~on client~ est√° delegando autorizaciones y permisos al server en lugar de directo al comando ~copy~.
   - ~using delimiters ',',E'\r'~ es para indicarle que los separadores de los campos son comas, y los separadores de l√≠nea es el caracter ~\r~, que significa _carriage return_.
   - ~null as ' '~ es para indicar que los strings vac√≠os deben ser considerados nulos.
2. Estos comandos son muy picky, y dentro de nuestro archivo, los caracteres especiales como vocales acentuadas, est√°n representadas con la clave _unicode_ ~<U+XXXX>~, donde ~XXXX~ es una clave en hexadecimal indicando el caracter. Por ejemplo, la delegaci√≥n "√Ålvaro Obreg√≥n" est√° dada como ~<U+00C1>lvaro Obreg<U+00F3>n~.
3. Esta notaci√≥n confunde al comando ~copy~ de MonetDB, y lo vuelve inoperante, a veces reportando que no existen valores en la columna 41 y l√≠nea 1, y a veces en la columna 2 y l√≠nea 1.
4. Encima de esto, el encoding de archivos puede ser un problema. Los archivos de texto, sean CSV o TXT, est√°n llenos de caracteres escondidos que le dan forma. Los caracteres escondidos m√°s comunes son los que representan _new line_, desafortunadamente son diferentes dependiendo del sistema operativo, y esto tambi√©n contribuye a que el ~copy~ no procese bien archivos de entrada:
   - ~\n~ para Linux y Mac
   - ~\r\n~ para Windows 10
   - ~\r~ para Windows 8 para atr√°s
5. Estamos hablando de un archivo de 45M de l√≠neas, as√≠ que cualquier intento de arreglar los problemas descritos arriba con ~sed~ o ~awk~ resultar√° en un tiempo de espera bastante largo.


** Las carreritas

Vamos a ejecutar un query anal√≠tico que obtenga el promedio de duraci√≥n de viajes entre todos los pares de colonias.

*En PostgreSQL üêò*

#+begin_src sql
  explain analyze select avg(eh.fecha_arribo_completa::timestamp - eh.fecha_retiro_completa::timestamp)::interval
    from ecobici_historico eh
    group by eh.colonia_retiro , eh.colonia_arribo;
#+end_src

Lo comenc√© a ejecutar alrededor de las 10 de la ma√±ana. Para las 2h transcurridas a√∫n no terminaba:


Decid√≠ interrumpirlo para intentar reducirlo en carga agreg√°ndole un ~WHERE~:

#+begin_src sql
  explain analyze select avg(eh.fecha_arribo_completa::timestamp - eh.fecha_retiro_completa::timestamp)::interval
    from ecobici_historico eh
    where eh.colonia_retiro = 'Cuauhtemoc'
    group by eh.colonia_retiro , eh.colonia_arribo;
#+end_src

Con este cambio tard√≥ *1 min 51 seg*:


Vamos a ver si le ganamos tantito con un √≠ndice sobre ~colonia_retiro~ dado que tenemos una condici√≥n ~where~:

#+begin_src sql
  create index big_data_ecobici_colonia_retiro on ecobici.ecobici_historico_import (
    colonia_retiro
  );
#+end_src

La creaci√≥n de √≠ndices igual es costosa en una tabla con millones de registros. Esta creaci√≥n se tard√≥ *2m 18s*.

Esta ejecuci√≥n tard√≥ *1m 50s* con un √≠ndice en el campo del ~WHERE~.

Le ganamos 1 seg ü§°ü§°ü§°

La raz√≥n de esto es que el query en particular est√° agrupando por 2 campos, y esto provoca un _sequential scan_, que es donde est√° el grueso del tiempo de la consulta.

En general, no es muy efectivo el √≠ndice.

*En MonetDB üñºÔ∏è*

El query en MonetDB tiene algunos cambios en sintaxis y no estamos agregando cl√°usula ~WHERE~ porque precisamente deseamos "presumir" las capacidades de las BDs columnares:

#+begin_src sql
  select eh.colonia_retiro , eh.colonia_arribo ,
         avg(cast(fecha_arribo_completa as timestamp) - cast(eh.fecha_retiro_completa as timestamp))/60 as promedio_duracion
    from ecobici_historico eh
   group by eh.colonia_retiro , eh.colonia_arribo
   order by promedio_duracion desc;
#+end_src

* C√≥mo usamos MonetDB como Data Warehouse?

El uso principal de las BDs columnares es como Data Warehouse.

El Data Warehousing es precisamente jalar de una relacional/transaccional y guardar en una columnar/anal√≠tica para formar hist√≥rico profundo.

Las caracter√≠sticas principales del Data Warehousing moderno son:
1. Los datos a cargar est√°n en forma de Big Table
2. La llave primaria de dicha Big Table es una columna que describe el paso del tiempo (a√∫n cuando no tengamos datos en cierto timeslot)

All√° afuera se van a encontrar a√∫n con gente que usa esquemas de _snowflake_ o _star_ para modelar data warehouses.

Ambos esquemas usan un dise√±o donde al centro est√° una tabla de _facts_ junto con las fechas, y decenas de llaves for√°neas, y alrededor, export√°ndoles su llave, decenas de tablas llamadas _dimensiones_, que son b√°sicamente los objetos de negocio.


#+DOWNLOADED: screenshot @ 2022-10-20 14:39:09
[[file:images/20221020-143909_screenshot.png]]

üëÄOJOüëÄ F√≠jense como este esquema se parece un buen a los esquemas relacionales que usualmente tenemos en las BDs relacionales/transaccionales.

Las _"dimensiones"_ son los *objetos de negocio*.

Los _"facts"_ son los *eventos de negocio* que combinan uno o m√°s objetos de negocio para describirse.

Y como tal, los _"facts"_ tienen como llave la _"dimensi√≥n"_ üï∞Ô∏è*TIEMPO*üï∞Ô∏è.

Estos esquemas de _dimensional modeling_ fueron creados por [[https://en.wikipedia.org/wiki/Dimensional_modeling][Ralph Kimball en el 96]], PERO en ese momento la realidad era muy, muy diferente.

Algunos supuestos de esos a√±os, que ya no son vigentes, son:

1. Databases are slow and expensive
2. SQL language is limited
3. You can never join fact tables because of one to many or many to one joins
4. Businesses are slow to change

Entonces, dado que:

1. Las bases de datos ya son r√°pidas y el storage barat√≠simo
2. Y que el SQL ha evolucionado a un lenguaje rico en features y expresiones que, aunque no forman parte del est√°ndar, nos simplifican la vida
3. Y que estas restricciones quedan acotadas en las bases de datos relacionales y que ya tenemos otras tantas formas de organizar data
4. Y que el mundo startupero ha redefinido la velocidad con la que se operan los negocios

Entonces podemos decir que el trabajo de Kimball es ya poco relevante.

Aunque cientos de ingenieros viejitos en el sector p√∫blico (y uno que otro del sector privado) les digan que no.

Lo √∫nico rescatable que podemos sacar del trabajo de Kimball es el manejo de la *dimensi√≥n tiempo*, que podemos combinar con esquemas modernos de _Big Table_ o _One Big Table_.

Vamos a utilizar la BD de Northwind para emular la creaci√≥n de un DWH con la dimensi√≥n tiempo:

** 1. Definir granularidad

Vamos a explorar las tablas centrales de la BD de Northwind para tratar de obtener la *frecuencia m√≠nima* con la que se crean nuevos registros en ellas.

- Las tablas centrales para el negocio de Northwind, *y que adem√°s tienen alg√∫n campo tipo ~date~* son:
  - ~orders~
  - ~employees~

- En la tabla ~orders~ tenemos que hay un nuevo registro cada *.8 d√≠as*
#+begin_src sql
  select avg(timediff) from
                         (SELECT order_date - lag(order_date) OVER (ORDER BY order_date) as timediff
                            FROM orders o
                           ORDER BY order_date) as t;
#+end_src
- En la tabla ~employees~ tenemos que hay un nuevo hire cada *150 d√≠as*
#+begin_src sql
  select avg(abs(timediff)) from
                              (SELECT hire_date - lag(hire_date) OVER (ORDER BY e.employee_id) as timediff
                                 FROM employees e
                                ORDER BY e.employee_id) as t;
#+end_src

Con esto podemos decir que la m√≠nima frecuencia de inserci√≥n es de 1 d√≠a.

Por tanto, la dimensi√≥n _time_ de nuestra BD hist√≥rica ser√° *diaria*:

** 2. Crear tabla con _dimensi√≥n tiempo_

Del lado de la BD fuente vamos a crear la tabla que representar√° nuestra dimensi√≥n de tiempo.

Vamos a ir a la fecha m√≠nima y m√°xima de las 2 tablas de arriba:

- En ~orders~ la m√≠nima de ~order_date~ es *1996-07-04* y el m√°ximo es *1998-05-06*
- En ~employees~ el m√≠nimo de ~hire_date~ es *1992-04-01* y el m√°ximo es *1994-11-15*

Por tanto, nuestra tabla con la dimensi√≥n de tiempo va a ir *diario* desde *1992-04-01* hasta  *1998-05-06*.

Esta tabla la vamos a crear del lado de PostgreSQL:

#+begin_src sql
  create table time_dimension (
    date_axis date primary key,
    seq_num serial unique not null
  );

  insert into time_dimension(date_axis) -- recordemos que para insertar desde un select, omitimos el keyword values
  select t.day::date
    from generate_series(timestamp '1992-04-01',
                         timestamp '1998-05-06',
                         interval '1 day') as t(day);
#+end_src

** 3. Extraer y hacer ~join~ con dimensi√≥n de tiempo

Ya con la tabla que nos da el eje de tiempo, podemos hacer las extracciones de toda la BD y hacer un ~left join~ con la tabla de tiempo para indicar cuando no hay evento en esa fecha para X o Y objeto de negocio:

#+begin_src sql
  select *
    from time_dimension td
         left outer join orders o on (td.date_axis = o.order_date)
         left outer join employees e on (td.date_axis = e.hire_date)
         left outer join order_details od using (order_id)
         left outer join products p using (product_id)
         left outer join categories cat using (category_id)
         left outer join suppliers s using (supplier_id)
         left outer join shippers sh on (o.ship_via = sh.shipper_id)
         left outer join customers cus using (customer_id)
   order by td.date_axis;
#+end_src

Algunas notas:

1. Por legibilidad, primero hacer el ~join~ entre la tabla de dimensi√≥n de tiempo y las tablas a las que vamos a sujetar a este eje com√∫n.
2. Usar ~left outer join~ para permitir nulos, y con esto, saber cuando en una fecha no tenemos ni _facts_ o *eventos* de ~employees~ u ~orders~.
3. Siempre ordenar (de forma ~asc~ o ~desc~) el query.

Pareciera que podemos insertar ya esta tabla de PostgreSQL a MonetDB, pero forzar un mismo eje o dimensi√≥n de tiempo en esta _big table_ nos pone demasiados nulos, que adem√°s est√°n localizados en un per√≠odo en espec√≠fico, y donde adem√°s hay poco empalme entre ambos per√≠odos.

Cuando los resultados son as√≠ de confusos, es recomendable entonces crear 2 tablas de _facts_ en nuestro DWH. En este caso, vamos a crear una tabla de _facts_ para ~orders~ y otra tabla de _facts_ para ~employees~:

#+begin_src sql
  select *
    from time_dimension td
         left outer join orders o on (td.date_axis = o.order_date)
         left outer join order_details od using (order_id)
         left outer join products p using (product_id)
         left outer join categories cat using (category_id)
         left outer join suppliers s using (supplier_id)
         left outer join shippers sh on (o.ship_via = sh.shipper_id)
         left outer join customers cus using (customer_id)
         left outer join employees e using (employee_id)
         left outer join employee_territories et using (employee_id)
         left outer join territories t using (territory_id)
   order by td.date_axis;

  select *
    from time_dimension td
         left outer join employees e on (td.date_axis = e.hire_date)
         left outer join employee_territories et using (employee_id)
         left outer join territories t using (territory_id)
   order by td.date_axis;
#+end_src

üëÄOJOüëÄ en ambas tablas de _facts_ tenemos info repetida sobre los empleados. Esto es perfectamente normal en el dise√±o de _Big Table_, dado que las 2 tablas sirven prop√≥sitos anal√≠ticos diferentes: mientras que los ~employees~ dentro de la 1a tabla son *dependientes* de ~order~, en la otra tabla de _facts_ los ~employees~ son la entidad central y solo los tenemos a ellos.

** 4. Copiar dichas tablas a MonetDB

Primero debemos crear las tablas para luego escribir estos datos.

Para esto vamos a usar una versi√≥n del comando ~create table~ que toma como entrada un ~as select...~.

#+begin_src sql
  create table fact_orders as
    select
      territory_id,
      employee_id,
      customer_id,
      supplier_id,
      category_id,
      product_id,
      order_id,
      date_axis,
      seq_num,
      order_date,
      required_date,
      shipped_date,
      ship_via,
      freight,
      ship_name,
      ship_address,
      ship_city,
      ship_region,
      ship_postal_code,
      ship_country,
      p.unit_price as unit_price_in_product,
      quantity,
      discount,
      product_name,
      quantity_per_unit,
      od.unit_price as unit_price_in_order,
      units_in_stock,
      units_on_order,
      reorder_level,
      discontinued,
      category_name,
      description,
      picture,
      s.company_name as supplier_company_name,
      s.contact_name,
      s.contact_title,
      s.address as supplier_address,
      s.city as supplier_city,
      s.region as supplier_region,
      s.postal_code,
      s.country as supplier_country,
      s.phone as supplier_phone,
      s.fax as supplier_fax,
      s.homepage,
      shipper_id,
      sh.company_name as shipper_company_name,
      sh.phone,
      cus.company_name,
      cus.contact_name as customer_company_name,
      cus.contact_title as customer_contact_title,
      cus.address as customer_address,
      cus.city as customer_city,
      cus.region as customer_region,
      cus.postal_code as customer_postal_code,
      cus.country as customer_country,
      cus.phone as customer_phone,
      cus.fax as customer_fax,
      e.last_name,
      e.first_name,
      e.title,
      e.title_of_courtesy,
      e.birth_date,
      e.hire_date,
      e.address as employee_address,
      e.city as employee_city,
      e.region as employee_region,
      e.postal_code as employee_postal_code,
      e.country as employee_country,
      e.home_phone,
      e.extension,
      e.photo,
      e.notes,
      e.reports_to,
      e.photo_path,
      t.territory_description,
      t.region_id
      from time_dimension td
           left outer join orders o on (td.date_axis = o.order_date)
           left outer join order_details od using (order_id)
           left outer join products p using (product_id)
           left outer join categories cat using (category_id)
           left outer join suppliers s using (supplier_id)
           left outer join shippers sh on (o.ship_via = sh.shipper_id)
           left outer join customers cus using (customer_id)
           left outer join employees e using (employee_id)
           left outer join employee_territories et using (employee_id)
           left outer join territories t using (territory_id)
     order by td.date_axis;
#+end_src

üëÄOJOüëÄ que, atendiendo al dicho "el flojo y el mezquino andan 2 veces el camino", tuve que quitar el ~*~ y hacer la talacha de desambiguar las columnas que se llamaban igual, pero estaban en diferentes tablas, usando alias.

Esto nos crea la tabla ~facts_orders~ en PostgreSQL que luego podemos mover a MonetDB.

Noten que la tabla no tiene primary key, y esto est√° correcto. Ya cuando vamos a mover a MonetDB, podemos generar otra llave, o usar el campo ~seq_num~ que viene desde la tabla de dimensi√≥n de tiempo.

#+begin_src sql
  CREATE TABLE fact_orders (
    territory_id varchar(20) NULL,
    employee_id int2 NULL,
    customer_id bpchar NULL,
    supplier_id int2 NULL,
    category_id int2 NULL,
    product_id int2 NULL,
    order_id int2 NULL,
    date_axis date NULL,
    seq_num int4 NULL,
    order_date date NULL,
    required_date date NULL,
    shipped_date date NULL,
    ship_via int2 NULL,
    freight float4 NULL,
    ship_name varchar(40) NULL,
    ship_address varchar(60) NULL,
    ship_city varchar(15) NULL,
    ship_region varchar(15) NULL,
    ship_postal_code varchar(10) NULL,
    ship_country varchar(15) NULL,
    unit_price_in_product float4 NULL,
    quantity int2 NULL,
    discount float4 NULL,
    product_name varchar(40) NULL,
    quantity_per_unit varchar(20) NULL,
    unit_price_in_order float4 NULL,
    units_in_stock int2 NULL,
    units_on_order int2 NULL,
    reorder_level int2 NULL,
    discontinued int4 NULL,
    category_name varchar(15) NULL,
    description text NULL,
    picture bytea NULL,
    supplier_company_name varchar(40) NULL,
    contact_name varchar(30) NULL,
    contact_title varchar(30) NULL,
    supplier_address varchar(60) NULL,
    supplier_city varchar(15) NULL,
    supplier_region varchar(15) NULL,
    postal_code varchar(10) NULL,
    supplier_country varchar(15) NULL,
    supplier_phone varchar(24) NULL,
    supplier_fax varchar(24) NULL,
    homepage text NULL,
    shipper_id int2 NULL,
    shipper_company_name varchar(40) NULL,
    phone varchar(24) NULL,
    company_name varchar(40) NULL,
    customer_company_name varchar(30) NULL,
    customer_contact_title varchar(30) NULL,
    customer_address varchar(60) NULL,
    customer_city varchar(15) NULL,
    customer_region varchar(15) NULL,
    customer_postal_code varchar(10) NULL,
    customer_country varchar(15) NULL,
    customer_phone varchar(24) NULL,
    customer_fax varchar(24) NULL,
    last_name varchar(20) NULL,
    first_name varchar(10) NULL,
    title varchar(30) NULL,
    title_of_courtesy varchar(25) NULL,
    birth_date date NULL,
    hire_date date NULL,
    employee_address varchar(60) NULL,
    employee_city varchar(15) NULL,
    employee_region varchar(15) NULL,
    employee_postal_code varchar(10) NULL,
    employee_country varchar(15) NULL,
    home_phone varchar(24) NULL,
    "extension" varchar(4) NULL,
    photo bytea NULL,
    notes text NULL,
    reports_to int2 NULL,
    photo_path varchar(255) NULL,
    territory_description bpchar NULL,
    region_id int2 NULL
  );
#+end_src
